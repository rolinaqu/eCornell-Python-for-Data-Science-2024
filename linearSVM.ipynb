{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ddd45a687908970d145ff08981192c68",
     "grade": false,
     "grade_id": "cell-5352bee401322381",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>About this Project</h2>\n",
    "<p>In this project, you will implement a linear support vector machine. You will generate a linearly separable dataset, write functions to build a linear SVM, and then visualize the decision boundary. You will also have the chance to add data points to a visualization of your linear SVM to see how it responds to new data.</p>\n",
    "\n",
    "<h3>Evaluation</h3>\n",
    "\n",
    "<p><strong>This project must be successfully completed and submitted in order to receive credit for this course. Your score on this project will be included in your final grade calculation.</strong><p>\n",
    "    \n",
    "<p>You are expected to write code where you see <em># YOUR CODE HERE</em> within the cells of this notebook. Not all cells will be graded; code input cells followed by cells marked with <em>#Autograder test cell</em> will be graded. Upon submitting your work, the code you write at these designated positions will be assessed using an \"autograder\" that will run all test cells to assess your code. You will receive feedback from the autograder that will identify any errors in your code. Use this feedback to improve your code if you need to resubmit. Be sure not to change the names of any provided functions, classes, or variables within the existing code cells, as this will interfere with the autograder. Also, remember to execute all code cells sequentially, not just those you’ve edited, to ensure your code runs properly.</p>\n",
    "    \n",
    "<p>You can resubmit your work as many times as necessary before the submission deadline. If you experience difficulty or have questions about this exercise, use the Q&A discussion board to engage with your peers or seek assistance from the instructor.<p>\n",
    "\n",
    "<p>Before starting your work, please review <a href=\"https://s3.amazonaws.com/ecornell/global/eCornellPlagiarismPolicy.pdf\">eCornell's policy regarding plagiarism</a> (the presentation of someone else's work as your own without source credit).</p>\n",
    "\n",
    "<h3>Submit Code for Autograder Feedback</h3>\n",
    "\n",
    "<p>Once you have completed your work on this notebook, you will submit your code for autograder review. Follow these steps:</p>\n",
    "\n",
    "<ol>\n",
    "  <li><strong>Save your notebook.</strong></li>\n",
    "  <li><strong>Mark as Completed —</strong> In the blue menu bar along the top of this code exercise window, you’ll see a menu item called <strong>Education</strong>. In the <strong>Education</strong> menu, click <strong>Mark as Completed</strong> to submit your code for autograder/instructor review. This process will take a moment and a progress bar will show you the status of your submission.</li>\n",
    "\t<li><strong>Review your results —</strong> Once your work is marked as complete, the results of the autograder will automatically be presented in a new tab within the code exercise window. You can click on the assessment name in this feedback window to see more details regarding specific feedback/errors in your code submission.</li>\n",
    "  <li><strong>Repeat, if necessary —</strong> The Jupyter notebook will always remain accessible in the first tabbed window of the exercise. To reattempt the work, you will first need to click <strong>Mark as Uncompleted</strong> in the <strong>Education</strong> menu and then proceed to make edits to the notebook. Once you are ready to resubmit, follow steps one through three. You can repeat this procedure as many times as necessary.</li>\n",
    "<p>You can also download a copy of this notebook in multiple formats using the <strong>Download as</strong> option in the <strong>File</strong> menu above.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "211b72917a61f0ac82464146be4bfad9",
     "grade": false,
     "grade_id": "cell-b3b882bc93b94a57",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h2>Getting Started</h2>\n",
    "<h3>Python Initialization</h3> \n",
    "\n",
    "Please run the following code to initialize your Python kernel. You should be running a version of Python 3.x. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're running python 3.6.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.append('/home/codio/workspace/.modules')\n",
    "from helper import *\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "import pylab\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "%matplotlib notebook\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "720ddd13e403e22808c68ee3e3c6135f",
     "grade": false,
     "grade_id": "cell-029db15eeaae00a8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Generate and Visualize Data</h3>\n",
    "\n",
    "Let's generate some linearly seperable data and visualize it in order to get a sense of what you will be working with. Run the cell below to generate and visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "763be17f85bb0326e12551ceefc009be",
     "grade": false,
     "grade_id": "cell-2827088afcbb8b23",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xTr, yTr = generate_data()\n",
    "visualize_2D(xTr, yTr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM\n",
    "\n",
    "Recall that the unconstrained loss function formulation for linear SVM is \n",
    "\n",
    "$$\n",
    "\\ell (\\mathbf{w}, b) = \\underbrace{\\mathbf{w}^\\top \\mathbf{w}}_{l_2 \\text{ regularizer}} +  C \\underbrace{\\sum_{i=1}^{n} \\max \\left[ 1 - y_{i} \\left( \\mathbf{w}^\\top \\mathbf{x}_i+b \\right),0 \\right]}_{\\text{hinge loss}}\n",
    "$$\n",
    "\n",
    "However, the hinge loss is not differentiable when $1-y_{i}\\left( \\mathbf{w}^\\top \\mathbf{x}_i+b \\right)= 0$. So, we are going to use the squared hinge loss instead:\n",
    "\n",
    "$$\n",
    "\\ell (\\mathbf{w}, b) = \\underbrace{\\mathbf{w}^\\top \\mathbf{w}}_{l_{2} \\text{ regularizer}} +  C \\underbrace{\\sum_{i=1}^{n} \\max \\left[ 1-y_{i} \\left( \\mathbf{w}^\\top \\mathbf{x}_i+b \\right),0 \\right]^2 }_{\\text{squared hinge loss}}\n",
    "$$\n",
    "\n",
    "### Part One: Loss Function [Graded]\n",
    "\n",
    "Firstly, implement the function **`loss`**, which takes in training data `xTr` ($n \\times d$) and labels `yTr` with `yTr[i]` either `-1` or `1` and calculates $\\ell(\\mathbf{w}, b)$ using the **squared hinge loss** for the classifier parameterized by `w, b` with hyperparameter `C`.\n",
    "\n",
    "You might find the function [`np.maximum(a, b)`](https://numpy.org/doc/stable/reference/generated/numpy.maximum.html) useful. It returns the element-wise maximum between each corresponding element of arrays `a` and `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "da998abb1d4c80d6654f2fa620024104",
     "grade": false,
     "grade_id": "cell-loss",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def loss(w, b, xTr, yTr, C):\n",
    "    \"\"\"\n",
    "    Calculates the squared hinge loss plus the l2 regularizer, as defined in the equation above.\n",
    "    \n",
    "    Input:\n",
    "        w     : d-dimensional weight vector\n",
    "        b     : bias term, a scalar\n",
    "        xTr   : nxd data matrix (each row is an input vector)\n",
    "        yTr   : n-dimensional vector (each entry is a label)\n",
    "        C     : scalar (constant that controls the tradeoff between l2-regularizer and hinge-loss)\n",
    "    \n",
    "    Output:\n",
    "        loss_val : squared loss plus the l2 regularizer for the classifier on xTr and yTr, a scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_val = 0.0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    wtxi = np.dot(w, xTr.T) + b\n",
    "    hingeloss = np.sum(np.maximum(1 - yTr * wtxi, 0)**2)\n",
    "    loss_val = np.dot(w.T, w) + C * hingeloss\n",
    "    return loss_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3a7da76228c57af30d57b7f168f3ef07",
     "grade": false,
     "grade_id": "cell-loss-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test: loss_test1 ... ✔ Passed!\n",
      "Running Test: loss_test2 ... ✔ Passed!\n",
      "Running Test: loss_test3 ... ✔ Passed!\n",
      "Running Test: loss_test4 ... ✔ Passed!\n",
      "Running Test: loss_test5 ... ✔ Passed!\n",
      "Running Test: loss_test6 ... ✔ Passed!\n"
     ]
    }
   ],
   "source": [
    "# These tests test whether your loss() is implemented correctly\n",
    "\n",
    "xTr_test, yTr_test = generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Check whether your loss() returns a scalar\n",
    "def loss_test1():\n",
    "    w = np.random.rand(d)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(w, b, xTr_test, yTr_test, 10)    \n",
    "    return np.isscalar(loss_val)\n",
    "\n",
    "# Check whether your loss() returns a nonnegative scalar\n",
    "def loss_test2():\n",
    "    w = np.random.rand(d)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(w, b, xTr_test, yTr_test, 10)\n",
    "    \n",
    "    return loss_val >= 0\n",
    "\n",
    "# Check whether you implement l2-regularizer correctly\n",
    "def loss_test3():\n",
    "    w = np.random.rand(d)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(w, b, xTr_test, yTr_test, 0)\n",
    "    loss_val_grader = loss_grader(w, b, xTr_test, yTr_test, 0)\n",
    "    \n",
    "    return (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "\n",
    "# Check whether you implemented the squared hinge loss and not the standard hinge loss\n",
    "# Note, loss_grader_wrong is the wrong implementation of the standard hinge-loss, \n",
    "# so the results should NOT match.\n",
    "def loss_test4():\n",
    "    w = np.random.randn(d)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(w, b, xTr_test, yTr_test, 1)\n",
    "    badloss = loss_grader_wrong(w, b, xTr_test, yTr_test, 1)    \n",
    "    return not(np.linalg.norm(loss_val - badloss) < 1e-5)\n",
    "\n",
    "\n",
    "# Check whether you implement square hinge loss correctly\n",
    "def loss_test5():\n",
    "    w = np.random.randn(d)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(w, b, xTr_test, yTr_test, 10)\n",
    "    loss_val_grader = loss_grader(w, b, xTr_test, yTr_test, 10)\n",
    "    \n",
    "    return (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "\n",
    "# Check whether you implement loss correctly\n",
    "def loss_test6():\n",
    "    w = np.random.randn(d)\n",
    "    b = np.random.rand(1)\n",
    "    loss_val = loss(w, b, xTr_test, yTr_test, 100)\n",
    "    loss_val_grader = loss_grader(w, b, xTr_test, yTr_test, 100)\n",
    "    \n",
    "    return (np.linalg.norm(loss_val - loss_val_grader) < 1e-5)\n",
    "\n",
    "runtest(loss_test1,'loss_test1')\n",
    "runtest(loss_test2,'loss_test2')\n",
    "runtest(loss_test3,'loss_test3')\n",
    "runtest(loss_test4,'loss_test4')\n",
    "runtest(loss_test5,'loss_test5')\n",
    "runtest(loss_test6,'loss_test6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e8a8bf4f84c63257cc5027f369297d0c",
     "grade": true,
     "grade_id": "cell-loss-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8c5cb3e3eadf76dd90def267e635eb40",
     "grade": true,
     "grade_id": "cell-loss-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e9e6086392f22c50444dadf2159d2c3a",
     "grade": true,
     "grade_id": "cell-loss-test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0ada7379e6811b8d2ed8c9a13a469f08",
     "grade": true,
     "grade_id": "cell-loss-test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "02b8b483e19d5c129f7cc8f2c3cd495a",
     "grade": true,
     "grade_id": "cell-loss-test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f13edb35d0e08461cb1444cadd40b52a",
     "grade": true,
     "grade_id": "cell-loss-test6",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs loss test5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "374895dc50bad19a1341a5711fdb7ba1",
     "grade": false,
     "grade_id": "cell-589f0f3cc664085a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Part Two: Gradient of Loss Function [Graded]\n",
    "\n",
    "Now, implement **`grad`**, which takes in the same arguments as the `loss` function but returns gradients of $\\ell$ (with squared hinge loss) with respect to $\\mathbf{w}$ and $b$.\n",
    "\n",
    "Here are the equations for your reference:\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{w}}\\ell = 2 \\mathbf{w} - 2C  \\sum_{i=1}^{n} y_i \\mathbf{x}_i \\max \\left[ 1 - y_{i} \\left(\\mathbf{w}^\\top \\mathbf{x}_i+b \\right), 0 \\right]\n",
    "$$\n",
    "$$\n",
    "\\nabla_b \\ell = -2C \\sum_{i=1}^{n} y_i \\max \\left[ 1-y_{i} \\left(\\mathbf{w}^\\top \\mathbf{x}_i+b \\right), 0 \\right]\n",
    "$$\n",
    "\n",
    "You might be able to reuse some of the code from your implementation of `loss` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4d0e49bfa25d8b3253ad7504c624e879",
     "grade": false,
     "grade_id": "cell-grad",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def grad(w, b, xTr, yTr, C):\n",
    "    \"\"\"\n",
    "    Calculates the gradients of the loss function as given by the expressions above.\n",
    "    \n",
    "    Input:\n",
    "        w     : d-dimensional weight vector\n",
    "        b     : bias term, a scalar\n",
    "        xTr   : nxd data matrix (each row is an input vector)\n",
    "        yTr   : n-dimensional vector (each entry is a label)\n",
    "        C     : constant (scalar that controls the tradeoff between l2-regularizer and hinge-loss)\n",
    "    \n",
    "    OUTPUTS:\n",
    "        wgrad, bgrad\n",
    "        wgrad : d-dimensional gradient of the loss with respect to the weight, w\n",
    "        bgrad : gradient of the loss with respect to the bias, b, a scalar\n",
    "    \"\"\"\n",
    "    n, d = xTr.shape\n",
    "    \n",
    "    wgrad = np.zeros(d)\n",
    "    bgrad = np.zeros(1)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    hingeloss = np.maximum((1- yTr * (np.dot(w, xTr.T) + b)), 0)\n",
    "    \n",
    "    wgrad = 2*w - 2*C * np.sum((hingeloss*yTr).reshape(-1, 1)*xTr, axis=0)\n",
    "    bgrad = -2*C * np.sum(yTr * hingeloss)\n",
    "    \n",
    "    return wgrad, bgrad\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2962c15a6109bd35e3e0e14dbfcdf3ce",
     "grade": false,
     "grade_id": "cell-grad-selftest",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Test: grad_test1 ... ✔ Passed!\n",
      "Running Test: grad_test2 ... ✔ Passed!\n",
      "Running Test: grad_test3 ... ✔ Passed!\n",
      "Running Test: grad_test4 ... ✖ Failed!\n",
      " The output of your function does not match the expected output. Check your code and try again.\n",
      "Running Test: grad_test5 ... ✔ Passed!\n"
     ]
    }
   ],
   "source": [
    "# These tests test whether your grad() is implemented correctly\n",
    "\n",
    "xTr_test, yTr_test = generate_data()\n",
    "n, d = xTr_test.shape\n",
    "\n",
    "# Checks whether grad returns a tuple\n",
    "def grad_test1():\n",
    "    w = np.random.rand(d)\n",
    "    b = np.random.rand(1)\n",
    "    out = grad(w, b, xTr_test, yTr_test, 10)\n",
    "    return len(out) == 2\n",
    "\n",
    "# Checks the dimension of gradients\n",
    "def grad_test2():\n",
    "    w = np.random.rand(d)\n",
    "    b = np.random.rand(1)\n",
    "    wgrad, bgrad = grad(w, b, xTr_test, yTr_test, 10)\n",
    "    return len(wgrad) == d and np.isscalar(bgrad)\n",
    "\n",
    "# Checks the gradient of the l2 regularizer\n",
    "def grad_test3():\n",
    "    w = np.random.rand(d)\n",
    "    b = np.random.rand(1)\n",
    "    wgrad, bgrad = grad(w, b, xTr_test, yTr_test, 0)\n",
    "    wgrad_grader, bgrad_grader = grad_grader(w, b, xTr_test, yTr_test, 0)\n",
    "    return (np.linalg.norm(wgrad - wgrad_grader) < 1e-5) and \\\n",
    "        (np.linalg.norm(bgrad - bgrad_grader) < 1e-5)\n",
    "\n",
    "# Checks the gradient of the square hinge loss\n",
    "def grad_test4():\n",
    "    w = np.zeros(d)\n",
    "    b = np.random.rand(1)\n",
    "    wgrad, bgrad = grad(w, b, xTr_test, yTr_test, 1)\n",
    "    wgrad_grader, bgrad_grader = grad_grader(w, b, xTr_test, yTr_test, 1)\n",
    "    return (np.linalg.norm(wgrad - wgrad_grader) < 1e-5) and \\\n",
    "        (np.linalg.norm(bgrad - bgrad_grader) < 1e-5)\n",
    "\n",
    "# Checks the gradient of the loss\n",
    "def grad_test5():\n",
    "    w = np.random.rand(d)\n",
    "    b = np.random.rand(1)\n",
    "    wgrad, bgrad = grad(w, b, xTr_test, yTr_test, 10)\n",
    "    wgrad_grader, bgrad_grader = grad_grader(w, b, xTr_test, yTr_test, 10)\n",
    "    return (np.linalg.norm(wgrad - wgrad_grader) < 1e-5) and \\\n",
    "        (np.linalg.norm(bgrad - bgrad_grader) < 1e-5)\n",
    "\n",
    "runtest(grad_test1, 'grad_test1')\n",
    "runtest(grad_test2, 'grad_test2')\n",
    "runtest(grad_test3, 'grad_test3')\n",
    "runtest(grad_test4, 'grad_test4')\n",
    "runtest(grad_test5, 'grad_test5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e6a5e50adcf9780a271d1d05c561fdaf",
     "grade": true,
     "grade_id": "cell-grad-test1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9092ccd637ef0692daf56b2d73f025a4",
     "grade": true,
     "grade_id": "cell-grad-test2",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "49eef008dacc9d50d1e58f47be24cceb",
     "grade": true,
     "grade_id": "cell-grad-test3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f0f17b5b6aa3d7e8cd72fb7fc4601e18",
     "grade": true,
     "grade_id": "cell-grad-test4",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9868abc360f647a4d380736d02d8dc84",
     "grade": true,
     "grade_id": "cell-grad-test5",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell - worth 1 point\n",
    "# runs grad test5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "525d5d1cc68fc282cca2fb12e8c54725",
     "grade": false,
     "grade_id": "cell-64100f321542f75b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Obtain the Linear SVM</h3>\n",
    "\n",
    "By calling the following minimization routine implemented for you in the cell below, you will obtain your linear SVM. Since the objective also includes the $l_2$-regularizer, the output loss that you would see would not be 0. But you can subtract off the regularizer term $\\mathbf{w}^\\top \\mathbf{w}$ to check that your squared hinge loss is indeed close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Final Loss of your model is: 96340718047648672407290040859143316629674685890808140224001857032776607358732103731976407764290326625924624330964261667848125474560309472570163098409550763597503266035859456.0000\n",
      "The Final Squared Hinge Loss of your model is: 96313640339628881636449006453152665262680595122883098442479264643247778455101737013579884558756805282760213700322688836608298055468947558964089453010141145556327300685889536.0000\n"
     ]
    }
   ],
   "source": [
    "w, b, final_loss = minimize(objective=loss, grad=grad, xTr=xTr, yTr=yTr, C=1000)\n",
    "print('The Final Loss of your model is: {:0.4f}'.format(final_loss))\n",
    "print('The Final Squared Hinge Loss of your model is: {:0.4f}'.format(final_loss - np.dot(w, w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "337fb33537139e644044dc274f183a4e",
     "grade": false,
     "grade_id": "cell-21d2b446b00cd3aa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Visualize the Decision Boundary</h3>\n",
    "\n",
    "Now, let's visualize the decision boundary on our linearly separable dataset. Since the dataset is linearly separable,  you should obtain $0\\%$ training error with sufficiently large values of $C$ (e.g. $C>1000$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "visualize_classfier(xTr, yTr, w, b)\n",
    "\n",
    "# Calculate the training error\n",
    "err=np.mean(np.sign(xTr.dot(w) + b)!=yTr)\n",
    "print(\"Training error: {:.2f} %\".format (err*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "44f5e7e213d2a9112b4c497b7634df7d",
     "grade": false,
     "grade_id": "cell-65f2c28b9b2b1014",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "<h3>Interactive Demo</h3>\n",
    "\n",
    "Running the code below will create an interactive window where you can click to add new data points to see how your linear SVM will respond. There may be a significant delay between clicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f601d30bf58ea07beb6ca68c029dadab",
     "grade": false,
     "grade_id": "cell-4d7dfed9aa687af9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "Xdata = []\n",
    "ldata = []\n",
    "\n",
    "fig = plt.figure()\n",
    "details = {\n",
    "    'w': None,\n",
    "    'b': None,\n",
    "    'stepsize': 1,\n",
    "    'ax': fig.add_subplot(111), \n",
    "    'line': None\n",
    "}\n",
    "\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.title('Click to add positive point and shift+click to add negative points.')\n",
    "\n",
    "def updateboundary(Xdata, ldata):\n",
    "    global details\n",
    "    w_pre, b_pre, _ = minimize(objective=loss, grad=grad, xTr=np.concatenate(Xdata), \n",
    "            yTr=np.array(ldata), C=1000)\n",
    "    details['w'] = np.array(w_pre).reshape(-1)\n",
    "    details['b'] = b_pre\n",
    "    details['stepsize'] += 1\n",
    "\n",
    "def updatescreen():\n",
    "    global details\n",
    "    b = details['b']\n",
    "    w = details['w']\n",
    "    q = -b / (w**2).sum() * w\n",
    "    if details['line'] is None:\n",
    "        details['line'], = details['ax'].plot([q[0] - w[1],q[0] + w[1]],[q[1] + w[0],q[1] - w[0]],'b--')\n",
    "    else:\n",
    "        details['line'].set_ydata([q[1] + w[0],q[1] - w[0]])\n",
    "        details['line'].set_xdata([q[0] - w[1],q[0] + w[1]])\n",
    "\n",
    "\n",
    "def generate_onclick(Xdata, ldata):    \n",
    "    global details\n",
    "\n",
    "    def onclick(event):\n",
    "        if event.key == 'shift': \n",
    "            # add positive point\n",
    "            details['ax'].plot(event.xdata,event.ydata,'or')\n",
    "            label = 1\n",
    "        else: # add negative point\n",
    "            details['ax'].plot(event.xdata,event.ydata,'ob')\n",
    "            label = -1    \n",
    "        pos = np.array([[event.xdata, event.ydata]])\n",
    "        ldata.append(label)\n",
    "        Xdata.append(pos)\n",
    "        updateboundary(Xdata,ldata)\n",
    "        updatescreen()\n",
    "    return onclick\n",
    "\n",
    "\n",
    "cid = fig.canvas.mpl_connect('button_press_event', generate_onclick(Xdata, ldata))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e0ba4a931d0f85f6ce8935609b758127",
     "grade": false,
     "grade_id": "cell-4f3686978b449027",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Scikit-learn Implementation\n",
    "\n",
    "Scikit-learn also provides a [Linear SVM Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) implementation that is easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC(penalty='l2', loss='squared_hinge', C=1000, max_iter=1000, random_state=0)\n",
    "clf.fit(xTr, yTr)\n",
    "\n",
    "# Visualize classifier and calculate training error\n",
    "visualize_classfier(xTr, yTr, clf.coef_, clf.intercept_)\n",
    "\n",
    "err=np.mean(clf.predict(xTr) != yTr)\n",
    "print(\"Training error: {:.2f} %\".format (err*100))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
